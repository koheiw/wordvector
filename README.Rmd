---
output: github_document
editor_options: 
  chunk_output_type: console
---

```{r, echo=FALSE, message=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "##",
  fig.path = "man/images/"
)
```

# Wordvector: word and document vector models


<!-- badges: start -->

[![CRAN
Version](https://www.r-pkg.org/badges/version/wordvector)](https://CRAN.R-project.org/package=wordvector)
[![Downloads](https://cranlogs.r-pkg.org/badges/wordvector)](https://CRAN.R-project.org/package=wordvector)
[![Total
Downloads](https://cranlogs.r-pkg.org/badges/grand-total/wordvector?color=orange)](https://CRAN.R-project.org/package=wordvector)
[![R build
status](https://github.com/koheiw/wordvector/workflows/R-CMD-check/badge.svg)](https://github.com/koheiw/wordvector/actions)
[![codecov](https://codecov.io/gh/koheiw/wordvector/branch/master/graph/badge.svg)](https://app.codecov.io/gh/koheiw/wordvector)

<!-- badges: end -->

The **wordvector** package is developed to create word and document vectors using **quanteda**. This package currently supports word2vec ([Mikolov et al., 2013](http://arxiv.org/abs/1310.4546)), doc2vec ([Le, Q. V., & Mikolov, T., 2014](https://doi.org/10.48550/arXiv.1405.4053)) and latent semantic analysis ([Deerwester et al., 1990](https://doi.org/10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9)). 

## How to install

**wordvector** is available on CRAN.

```{r, eval=FALSE}
install.packages("wordvector")
```

The latest version is available on Github.

```{r, eval=FALSE}
remotes::install_github("koheiw/wordvector")
```


## Example

We train the word2vec model on a [corpus of news summaries collected from Yahoo News](https://www.dropbox.com/s/e19kslwhuu9yc2z/yahoo-news.RDS?dl=1) via RSS between 2012 and 2016. 

### Download data

```{r, eval=FALSE}
# download data
download.file('https://www.dropbox.com/s/e19kslwhuu9yc2z/yahoo-news.RDS?dl=1', 
              '~/yahoo-news.RDS', mode = "wb")
```

### Train word2vec

```{r}
library(wordvector)
library(quanteda)

# Load data
dat <- readRDS('~/yahoo-news.RDS')
dat$text <- paste0(dat$head, ". ", dat$body)
corp <- corpus(dat, text_field = 'text', docid_field = "tid")

# Pre-processing
toks <- tokens(corp, remove_punct = TRUE, remove_symbols = TRUE) %>% 
    tokens_remove(stopwords("en", "marimo"), padding = TRUE) %>% 
    tokens_select("^[a-zA-Z-]+$", valuetype = "regex", case_insensitive = FALSE,
                  padding = TRUE)

# Train word2vec
wov <- textmodel_word2vec(toks, dim = 50, type = "cbow", min_count = 5, verbose = TRUE)
```

### Similarity between word vectors

`similarity()` computes cosine similarity between word vectors.

```{r}
head(similarity(wov, c("amazon", "forests", "obama", "america", "afghanistan"), 
                mode = "character"))
```

### Arithmetic operations of word vectors

`analogy()` offers interface for arithmetic operations of word vectors. 

```{r}
# What is Amazon without forests?
head(similarity(wov, analogy(~ amazon - forests))) 
```

```{r}
# What is for Afghanistan as Obama for America? 
head(similarity(wov, analogy(~ obama - america + afghanistan))) 
```

These examples replicates analogical tasks in the original word2vec paper.

```{r}
# What is for France as Berlin for Germany?
head(similarity(wov, analogy(~ berlin - germany + france))) 
```

```{r}
# What is for slowly as quick for quickly?
head(similarity(wov, analogy(~ quick - quickly + slowly)))
```


